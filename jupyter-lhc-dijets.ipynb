{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A proof-of-principle Jupyter Notebook using ATLAS dijet data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces the reader to the interpretation of the ATLAS search for new dijet resonances beyond the Standard Model (BSM) done using proton-proton data taken at the LHC in 2015 and 2016 [https://arxiv.org/abs/1703.09127](Phys. Rev. D 96 2017 052004). \n",
    "After introducing the basic theory and the experimental setup, this Jupyter notebooks depicts different methods of comparing the data with a predicted background. This notebook guides the reader to make figures illustrating different ways to compare data and expectation, as in [arXiv:1111.2062](https://arxiv.org/abs/1111.2062). The notebook is programmed in ```Python3``` but can also be run on ```Python2```. \n",
    "\n",
    "This notebook has been originally created by Bachelor Student Leonie Hermann, supervised by Caterina Doglioni (Lund University, who also adapted it for the COMPUTE Jupyter course) and Markus Schumacher (University of Freiburg). The thesis can be found here: https://terascale.physik.uni-freiburg.de/Publikationen/abschlussarbeiten/bachelorarbeiten/bachelorLeonieHermann/view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory ##\n",
    "\n",
    "The following part picks up the most important basics which are necessary to understand the physics behind the dijet resonance search. It starts with the main parts of the Standard Model (SM). Then, jet production and the  search for new particles are introduced after the experimental tools. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Standard Model  ###\n",
    "\n",
    "The Standard Model (SM) was introduced in 1960 and describes the fundamental particles and their interactions through the fundamental forces. It classifies the elementary particles into classes and in three generations of matter called fermions and into the interaction particles called bosons. The fermions are divided into quarks and leptons. The quarks build up particles such as baryons and mesons - also called hadrons. There are three pairs of quarks: up and down, charm and strange, top and bottom. The three lepton generations include the electron, the muon and the tauon, their respectively antiparticle and their neutrinos and antineutrinos.\n",
    "The particles mediating the interactions are called gauge bosons or force carriers. The photon $\\gamma$ interacts electromagnetically, the $W^\\pm$ and $Z$ bosons interact weakly and the gluon interacts strongly. The recently discovered Higgs boson explains how most SM particles acquires mass. The SM particles are shown in the figure below. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/00/Standard_Model_of_Elementary_Particles.svg\" width=\"400\" />\n",
    "\n",
    "By MissMJ [CC BY 3.0 (https://creativecommons.org/licenses/by/3.0) or Public domain], via Wikimedia Commons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup ##\n",
    "\n",
    "### The Large Hadron Collider ###\n",
    "\n",
    "The Large Hadron Collider (LHC) is currently the largest and most powerful man-made circular particle accelerator in the world.  It is located at the border of France and Switzerland near Geneva in Switzerland and is part of CERN.  The $27.6\\,\\text{km}$ long accelerator has four collision points where experiments and detectors are placed. On of the largest experiment is the ATLAS (A Toroidal LHC ApparatuS) detector. [Note: the following cell shows how to embed a video.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('UDoIzvKumGI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ATLAS experiment ###\n",
    "\n",
    "Protons collide with a total center-of-mass energy of $13\\, \\text{TeV}$ $1$ billion times per second in correspondence of the ATLAS detector and other experiments. The new particles created from the collision are measured by different layer of the ATLAS detector, which is shown in the figure below.\n",
    "* Close to the center of collision, the inner detector is a pixel/semiconductor detector responsible for measuring the tracks of the particles. A superconducting solenoid magnet is necessary to measure the momentum of the particles. \n",
    "* Following the tracking detector, the electromagnetic calorimeter absorbs the particles which interact electromagnetically. The momentum and energy of those particles are measured by collecting the energy from electromagnetic showers.\n",
    "* The following calorimeter is called hadronic calorimeter. The interaction of particles with the strong force leads to their absorption and energy loss. An active material measures the energy from the so-called hadronic showers. \n",
    "* The outer layer is the muon spectrometer, outside toroidal magnets. The muon spectrometer measures the energy and momentum of the muons. \n",
    "\n",
    "<img src=\"http://mediaarchive.cern.ch/MediaArchive/Photo/Public/2008/0803012/0803012_01/0803012_01-A5-at-72-dpi.jpg\" width=\"600\" />\n",
    "\n",
    "Optional material: How the ATLAS experiment detects particles is described in the video below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('iYRQpcJVQx8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we looking for, in this notebook? ## \n",
    "\n",
    "### Jet Production in Proton-Proton Collisions ###\n",
    "\n",
    "When protons collide with each other in a _collision event_, one of the most frequent processes that is seen in the detector is the production of two particles coming from the initial protons. These particles are quarks and gluons. We cannot measure those particles directly as they soon turn into _cones_ of other particles in a process called hadronization. These cones are called jets, they consist of particles that interact with the strong force and carry a lot of the energy of the initial collision. A jet is a collimated flow of particles moving essential in the same direction. A _dijet event_ is the signature in the detector of two separate jets, generally back-to-back from each other. The distribution of the energy of those two jets combined (also called invariant mass, $m_{jj}$) is predicted by the Standard Model to be a monotonically decreasing, smooth function. \n",
    "\n",
    "\n",
    "### New Physics beyond the Standard Model ###\n",
    "The SM cannot explain all phenomena which has been observed. Those include for example dark matter (DM) and dark energy which compose most of the universe. Theories beyond the Standard Model predict mediator particles between the SM and DM. A new heavy particle which decays into dijets could be a DM mediator and then a huge progress in the search for dark matter.  \n",
    "\n",
    "\n",
    "### Resonance search ### \n",
    "\n",
    "When considering theories beyond the Standard Models with new particle states decaying into dijets, an excess in this $m_{jj}$ distribution could emerge on top of the smooth distribution of dijet events from QCD. The main part of this notebook focuses on the methods to visualize such an excess, once we have a prediction of the background from the Standard Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Background Prediction ##\n",
    "\n",
    "\n",
    "With the ATLAS detector at the Large Hadron Collider (LHC) at CERN in Switzerland and France, proton-proton collision events at a center of mass energy of $13\\,\\text{TeV}$ are investigated for the presence of new resonances decaying into dijets. The search includes data from the 2015 and 2016 runs (with an integrated luminosity of $37\\,\\text{fb}^{-1}$). \n",
    "\n",
    "[Advanced] The reconstruction and calibration of the jet is described in the thesis and summarized here. For the data analysis dijets events with the following criteria are chosen:\n",
    "* a leading jet with $p_T > 440\\,$GeV\n",
    "* a sub-leading jet with $p_T > 60\\,$GeV\n",
    "* $m_{jj} > 1\\,$TeV\n",
    "\n",
    "Since the background is smooth, it can be estimated using a fit to the data as long as this fit does not accommodate for local fluctuations. In the case of this search, the background is estimated by a fitting method called *Sliding Window Fits* (SWiFt). A three parameter fit function $f(x) = p_1 (1-x)^{p_2} x^{p_3}$ is used to fit the data in smooth function that does not allow local excesses. The technique fits the data in smaller ranges called windows which slides in overlapping steps in the spectrum. The fitted value of the bin in the center is then the background prediction. For the bins near the edge, the window is reduced to 60% to the nominal window size and all values of the fit are taken for the background fit. The background prediction has two types of uncertainties which are depicted later in the notebook. One uncertainty comes from the choice of the fit function e.g. taking instead a four-parameter fit function and the other uncertainty is due to determining the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The  Analysis ## \n",
    "\n",
    "In the analysis, the background prediction is  compared to the measured data. It is investigated in order to find localized excesses appearing as a bump on top of the smooth QCD distribution. Firstly, data and background prediction are compared using hypothesis tests and then visually investigated using the metric in [arXiv:1111.2062](https://arxiv.org/abs/1111.2062). This is described in *Methods to compare Data and Background Bin-by-Bin* and in the written elaboration.  Secondly, the hypothesis hyper tests BumpHunter and TailHunter algorithm are applied to the dataset.  \n",
    "\n",
    "The main parts of the analysis (Statistical significance and BumpHunter algorithm) were already performed by the ATLAS Collaboration. \n",
    "The comparison of the data and the background prediction from [this search](http://inspirehep.net/record/1519428/files/fig1a.png) are shown in the figure below. No significant local excesses are observed. \n",
    "\n",
    "<img src=\"http://inspirehep.net/record/1519428/files/fig1a.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Codes of Statistical Analysis ## \n",
    "\n",
    "### Introduction ###\n",
    "This is the main part of the *Proof of Principle Notebook*. The data and the background data from this search is made available [on the HEPData platform](https://www.hepdata.net/record/77265). Ensure, that table 1 is saved in the same directory as this notebook for an error-free operation. Each coding cell with [N] should be executed in the chronological  order by pressing shift-enter.\n",
    "\n",
    "\n",
    "For running this notebook in the interface JuypterLab, ```%matplotlib nbagg``` must be replaced by ```%matplotlib inline``` so that the figures appear below the coding cells. ```%matplotlib nbagg``` creates interactive figures where the range of the spectrum and the zoom can easily be adapted. \n",
    "\n",
    "\n",
    " With ```pandas```, the data is presented and read, the figures are done with ```matplotlib.pyplot``` and to calculate the significance, ```scipy``` and a formular from ```statsmodels.api``` are used.  Those are the main packages applied in this code and therefore are imported in the beginning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib nbagg    \n",
    "# %matplotlib inline # For JupyterLab usage\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'errorbar.capsize': 2})     # Make nicer errorbars\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import scipy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Data ###\n",
    "\n",
    "The following part imports the data from the .csv file called ```HEPData-ins1519428-v2-Table1.csv``` to the data frame called ```data``` and ```temp```. First, the measured data is presented using ```pandas``` tables. The second table shows the background prediction and the uncertainties. \n",
    "\n",
    "In  ```names=[...]```, the title of each column is defined. First, the binning parameters are given. The bin center  ```$m_{jj}$ [TEV]``` is followed by the left bin edge   ```$m_{jj}$ [TEV] LOW``` and the right bin edge ```$m_{jj}$ [TEV] HIGH``` which determine together the bin width. The last column gives the number of events ```Data [ Events/Bin ]``` for the corresponding bin. The second table presents again the binning which is followed by the background prediction ```Background [ Events/Bin ]``` and two different types of uncertainties  ```Sys Fit function``` and ```Sys Fit parameters```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"HEPData-ins1519428-v2-Table_1.csv\",  names=[\"$m_{jj}$ [TEV]\", \"$m_{jj}$ [TEV] LOW\", \"$m_{jj}$ [TEV] HIGH\", \"Data [ Events/Bin ]\"])\n",
    "#this is a vector of strings so far\n",
    "temp_data = pd.concat([data])  \n",
    "\n",
    "#you only want the rows with the data, so print it out to check which ones you want\n",
    "#print(temp)\n",
    "\n",
    "#now we turn the columns we want into numbers\n",
    "data_table = temp_data[10:102].astype('float64')\n",
    "\n",
    "#and then we print them\n",
    "data_table.style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting the Data ####\n",
    "\n",
    "This high energy physicist does not approve of your significant digits. How can you fix them? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"http://gate.hep.anl.gov/lecompte/Bio/koala.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first point of call, we can use the pandas \"round\" module:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.round.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this does not follow the rules & regulations for significant digits that you may have learned in various labs. A personal take (some people get rather religious about them) is to only use the significant digits that carry meaning to the precision of your experiment. So if you have an error on your measurement, you truncate the measurement according to the error. However, you also know the error with a limited precision (this can also be measured!), so where do you stop? For this course we'll stop with a convention: truncate the error to two digits, and round the measurement to a similar number of significant digits. \n",
    "\n",
    "The data that we have in those tables follows a probability distribution function that is \"Poisson\" (for a primer: https://www.umass.edu/wsp/resources/poisson/) and the statistical error on a number of counts N is sqrt(N). So, let's round the number of counts in the \"Data\" column separately, using the names in the pandas table above. The first three columns are simply the edges of the bins, while the fourth column will use information about itself to round. If you want to exercise some pandas/lambda wisdom, you can add a cell here and make the table look even nicer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = pd.read_csv(\"HEPData-ins1519428-v2-Table_1.csv\",  names=[\"$m_{jj}$ [TEV]\", \"$m_{jj}$ [TEV] LOW\", \"$m_{jj}$ [TEV] HIGH\", \"Background [ Events/Bin ]\", \"Sys Fit function + [ Events/Bin ]\", \"Sys Fit function - [ Events/Bin ]\", \"Sys Fit parameters + [ Events/Bin ]\",  \"Sys Fit parameters - [ Events/Bin ]\" ])\n",
    "temp_back = pd.concat([background])  \n",
    "back_table = temp_back[105:197].astype('float64')\n",
    "\n",
    "\n",
    "back_table.style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Step 1: Preparing data and the binning** We need columns in the tables above to turn into lists. Let's start again from the csv output. The first table with the measured data is imported to the following lists:\n",
    "```python\n",
    "x=[]   # Bin center of data\n",
    "y=[]   # Measured data\n",
    "\n",
    "# Bin edges\n",
    "widthlow = []   # Left edge\n",
    "widthhigh = []  # Right edge\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data in the tables above in the following steps:\n",
    "# Bin center (mjj in TeV)\n",
    "x = data[ \"$m_{jj}$ [TEV]\"][10:102]      # Taking only the corresponding rows in the file\n",
    "x = pd.Series(x).values.tolist()         # Converting to an array\n",
    "x = [float(i) for i in x]                # Converting to a list\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Measured Events/Bin\n",
    "y = [float(i) for i in pd.Series(data[\"Data [ Events/Bin ]\"][10:102]).values.tolist()]\n",
    "print(y)\n",
    "# Bin edges\n",
    "widthlow = [float(i) for i in pd.Series(data[\"$m_{jj}$ [TEV] LOW\"][10:102]).values.tolist()]\n",
    "widthhigh =  [float(i) for i in pd.Series(data[\"$m_{jj}$ [TEV] HIGH\"][10:102]).values.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Step 2: Preparing background and systematic uncertainties**\n",
    "In the second table, the background prediction is given with uncertainties. The bin center and edges are the same as in the first table. Therefore, they are not imported again.\n",
    "```python\n",
    "yb=[]   # Background prediction \n",
    "\n",
    "# Uncertainties\n",
    "# Systematic uncertainties due to the fit function\n",
    "fplus = []\n",
    "fminus =[]\n",
    "# Systematic uncertainties due to the choice of the parameters\n",
    "pplus = []\n",
    "pminus =[]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measured Events/Bin\n",
    "\n",
    "# Background prediction\n",
    "yb = [float(i) for i in pd.Series(background[\"Background [ Events/Bin ]\"][105:197]).values.tolist()]\n",
    "\n",
    "# Systematic uncertainties due to the fit function\n",
    "fplus = [float(i) for i in pd.Series(background[\"Sys Fit function + [ Events/Bin ]\"][105:197]).values.tolist()]\n",
    "fminus = [float(i) for i in pd.Series(background[\"Sys Fit function - [ Events/Bin ]\"][105:197]).values.tolist()]\n",
    "\n",
    "# Systematic uncertainties due to the choice of the parameters\n",
    "pplus = [float(i) for i in pd.Series(background[\"Sys Fit parameters + [ Events/Bin ]\"][105:197]).values.tolist()]\n",
    "pminus = [float(i) for i in pd.Series(background[\"Sys Fit parameters - [ Events/Bin ]\" ][105:197]).values.tolist()]\n",
    "\n",
    "# Creating additional list with only zeros in the length of bins.\n",
    "i = 0\n",
    "null = []\n",
    "while i< len(x):\n",
    "    null.append(float(0))\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Step 3: Preparing statistical uncertainties**\n",
    "A Poisson distribution is assumed for the measured data so that the statistical uncertainty is calculated by $\\sqrt{N}$ with $N$ as number of events. \n",
    "The data uncertainty is given by\n",
    "```python\n",
    "yderr=[].\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yderr = [yderr**(1./2) for yderr in y ]     # Poisson uncertainty of the y-value of the measured data\n",
    "#if you want to see the errors:\n",
    "print(yderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of Figures containing Data and Background Prediction ###\n",
    "\n",
    "Now, the data and the background prediction can be depicted including uncertainties. A proper figure includes\n",
    "\n",
    "* labeled axis \n",
    "\n",
    "* an appropriate scale (e.g.logarithmic)\n",
    "\n",
    "* a grid\n",
    "\n",
    "* a legend.\n",
    "\n",
    "How to add these details is shown in the following coding cells. First, only the background is shown with the two systematic uncertainties. Then, the data and the background prediction are shown in one figure. \n",
    "\n",
    "***1. Depicting background with the systematic uncertainties***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit of the background with uncertainties\n",
    "\n",
    "plt.rcParams.update({'errorbar.capsize': 1.5})\n",
    "backgroundonly = plt.figure()\n",
    "ax1 = plt.subplot(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7cdcf763619d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Label the axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'$m_{jj}$ in TeV'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Events / Bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Adding a grid (optional)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Label the axis\n",
    "plt.xlabel('$m_{jj}$ in TeV')\n",
    "plt.ylabel('Events / Bin')\n",
    "# Adding a grid (optional)\n",
    "plt.grid()\n",
    "\n",
    "# Plot the background prediction in a step function\n",
    "plt.step(x, yb, 'r', linewidth= 0.5, data=None, where='mid', label='Background fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the two types of systematic uncertainties to the step function\n",
    "\n",
    "# Detail: the uncertainty due to the choice of the fit function goes only in one direction (same sign in the table)\n",
    "ax1.errorbar(x,yb, yerr=(fplus, null), fmt='None',  ecolor='b', label='Uncertainty due to \\n  choice of background parametrization  ', markersize=2.1, elinewidth=0.8)\n",
    "# Detail: the uncertainty due to the choice of the parameters goes in both directions (different signs in the table)\n",
    "ax1.errorbar(x,yb, yerr=(pplus), fmt='None', ecolor='k',label='Uncertainties due to  \\n values of parameters', markersize=2.1, elinewidth=0.8)\n",
    "\n",
    "# Make the y-axis in logarithmic scale\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "# Display the legend\n",
    "plt.legend(loc=1)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('Backgroundonly.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The systematic uncertainties on the background are very small. Therefore, in the following figures, these uncertainties are neglected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** 2. Depicting data and background prediction***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit of the background with uncertainties\n",
    "databackground = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "# Label the axis\n",
    "plt.xlabel('$m_{jj}$ in TeV')\n",
    "plt.ylabel('Events / Bin')\n",
    "\n",
    "# Adding a grid (optional)\n",
    "plt.grid()\n",
    "\n",
    "# Plot the data with the statistical uncertainty\n",
    "ax.errorbar(x,y, yerr=yderr, fmt='ko', label='Data', markersize=2.1, elinewidth=0.8)\n",
    "\n",
    "# Plot the background prediction in a step function\n",
    "plt.step(x, yb, 'r', linewidth= 0.5, data=None, where='mid', label='Background fit')\n",
    "\n",
    "#(Optional) Add text with further information about the plot\n",
    "#ax.text(1,10**0+1,'Center of mass energy :$\\sqrt{s} = 13\\,$TeV')\n",
    "#ax.text(1,10**(-1)+ 10**(-0.5),'Integrated luminosity 37.0 fb$^{-1}$')\n",
    "#ax.text(1,10**(-1),'Run in 2015 and 2016')\n",
    "\n",
    "# Make the y-axis in logarithmic scale\n",
    "plt.yscale('log', nonposy='clip')  # 'clip' avoids artefacts in the plot, when the error band extends to negative values\n",
    "\n",
    "# (Optional) x-axis in logarithmic scale (two different ways)\n",
    "#plt.semilogx()\n",
    "#plt.xscale('log')\n",
    "\n",
    "# Make the legend\n",
    "plt.legend(loc=1)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Save the figure with the name... in the format...\n",
    "plt.savefig('HEPDataandbkg.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods to compare Data and Background Bin-by-Bin\n",
    "There are different ways to investigate the differences between data and expectations as presented in [arXiv:1111.2062](https://arxiv.org/abs/1111.2062). \n",
    "\n",
    "Before, the histogram can be plotted, some preparations must be done. One important part is to determine the width of the bins individually. In the histogram, every bin has a different width. The edges of the bin values are given in the second and third row of the tables shown in *Reading the Data*. Their differences determine the bin width ```width=[]```: ```width[i] = widthhigh[i]-widthlow[i]```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the width for each bin\n",
    "width=[]\n",
    "i=0\n",
    "while i < len(widthlow):                     # Calculate the width for all entries \n",
    "    width.append(widthhigh[i]-widthlow[i])   # The width is the difference of the high and the low edge\n",
    "    i=i+1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following methods are applied on the data and the background prediction: \n",
    "* Absolute difference\n",
    "* Relative difference\n",
    "* $\\frac{D-B}{\\sqrt{B}}$-approximation of the significance\n",
    "* Statistical significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Absolute and relative difference of data and background\n",
    "\n",
    "#### Absolute difference between data and background\n",
    "\n",
    "The first method to investigate the differences between data and background prediction is to take the absolute difference ```absolute=```$\\Delta E(i)$ \n",
    "\\begin{equation}\n",
    "    \\Delta E(i)=D(i)-B(i)\n",
    "\\end{equation}\n",
    "at each bin $i$. The number of events in the data set is ```y=```$D(i)$ and in the background model ```yb=```$B(i)$. An excess has a positive and a deficit a negative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the values of the absolute difference\n",
    "absolute=[]\n",
    "i=0\n",
    "while i < len(x):                # Calculate the abs. difference for all bins\n",
    "    absolute.append(y[i]-yb[i])  # Subtract the background from the measured data value\n",
    "    i=i+1\n",
    "\n",
    "# Plot the absolute difference in  a histogram\n",
    "figabsdiff = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "# Label the axis \n",
    "plt.xlabel('$m_{jj}$ in TeV')\n",
    "plt.ylabel('$\\Delta$ Events / Bin')\n",
    "\n",
    "# Make a grid \n",
    "plt.grid()\n",
    "\n",
    "# Plot the bars in the histogram\n",
    "ax.bar(x, absolute, width=width, color='red', edgecolor=['crimson']*len(x))\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('Absolutediff.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The absolute difference is usually high for high populated bins  and  low for bins with very low population. This method does not give any useful information about how the data set is in accordance with the background model. It does not show any significant deviations for data counts, which span over several orders of magnitude. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result: Relative Difference between Data and Background\n",
    "One other way to investigate the differences, is to take the ratio of data over expectation\n",
    "$$ \\frac{\\text{Data}}{\\text{Expected}} = \\frac{D}{B}$$\n",
    "and calculate the relative difference ```rel```\n",
    "$$ \\frac{D}{B}- 1 = \\frac{D-B}{B}.$$\n",
    "$\\Rightarrow$ ``` rel[i] = (y[i]-yb[i])/yb[i]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of the relative difference for all bins \n",
    "rel = []  \n",
    "i=0\n",
    "while i < len(y):\n",
    "    rel.append((y[i]-yb[i])/yb[i])    # Adding the relative difference of each bin to the list 'rel'.\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative difference for low values of $m_{jj}$ is almost zero. For $m_{jj}> 4\\,$TeV, the absolute value of the relative difference rises. Deviations in form of an excess as well as a deficit appear. The highest relative difference is present at $m_{jj} = 8\\,$TeV for two adjacent bins with an excess of over 600% and 800%.\n",
    "\n",
    "This way of plotting does not show how significant the deviations are with respect to the relative uncertainty. For high -populated bins, significant discrepancies can be hidden since the relative difference becomes smaller for a large background. The fluctuations of the relative difference rise for low-populated bins. \n",
    "It seems that most fluctuations are observed at the low-populated bins. \n",
    "This is the deficit of the method and has a big impact if the data set spans over several orders of magnitude.\n",
    "This method does not statistically quantify the agreement between data and background prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the relative difference\n",
    "figrel = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "# Label the axis\n",
    "plt.xlabel('$m_{jj}$ in TeV')\n",
    "plt.ylabel('(D-B) / B')\n",
    "\n",
    "# Add a grid\n",
    "plt.grid()\n",
    "\n",
    "# Plot the bars in the histogram\n",
    "ax.bar(x, rel, width=width, color='orange', edgecolor=['gold']*len(x))\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('RelDifferencehisto.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The $\\frac{D-B}{\\sqrt{B}}$-approximation\n",
    "\n",
    "Another possibility is to consider the $\\frac{D-B}{\\sqrt B}$-approximation. \n",
    "The Poisson distribution approximates the Gaussian distribution for a high number of events in each bin. The statistical significance (also called z-value, intuitively corresponding to the number of standard deviations by which the value of an observation is above the mean value of that observation) becomes then\n",
    "\\begin{equation}\n",
    "    \\text{zvalue}  = \\frac{x-\\mu}{\\sigma} = \\frac{x-\\mu}{\\sqrt{\\mu}}.\n",
    "\\end{equation}\n",
    "with a standard deviation of $\\sigma={\\sqrt{\\mu}}$. \n",
    "Applying this to the example with the measured data $D$ and the estimated background $B$, the significance  can be approximated by\n",
    "\\begin{align}\n",
    "    \\text{zvalue}= \\frac{D-B}{\\sqrt{B}}\n",
    "\\end{align}\n",
    " $\\Rightarrow$ ```app[i] = (y[i]-yb[i])/(yb[i]**(0.5)) ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Gaussian approximation of the significance (D-B)/(B)^(1/2)\n",
    "\n",
    "# List for the values of the approximation\n",
    "app=[]\n",
    "i=0\n",
    "while i < len(y):\n",
    "    app.append((y[i]-yb[i])/(yb[i]**(0.5)))  # Calculation and adding the values to the list\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result: $\\frac{D-B}{\\sqrt B}$ - approximation\n",
    "A short code is applied which separates the bins with an excess ```plus = []``` to the bins with a deficit ```minus = [] ```. The excesses are shown in green and the deficits in red. The following code adds all negative values of the approximation to the list ```minus = [] ``` and all positive to ```plus = []```. Then, the both lists are used to create a figure. Different colors are chosen to highlight the differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All deficits are added to the list called 'minus'\n",
    "minus=[]\n",
    "i=0\n",
    "while i < len(y):\n",
    "    if app[i]<0:\n",
    "        minus.append(app[i])      # Add all negative values to the list\n",
    "    else:\n",
    "        minus.append(0)           # Positive values are set to zero to keep the positions in the list\n",
    "    i=i+1\n",
    "\n",
    "\n",
    "# All excesses are added to the list 'plus'\n",
    "plus=[]\n",
    "i=0\n",
    "while i < len(y):\n",
    "    if app[i]>0:\n",
    "        plus.append(app[i])     # Add all positive values to the list\n",
    "    else:\n",
    "        plus.append(0)          # Negative values are set to zero to keep the positions in the list   \n",
    "    i=i+1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Gaussian approximation of the significance (D-B)/(B)^(1/2) and make the excesses in another color than the deficits\n",
    "figapp = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "# Label the axis\n",
    "plt.xlabel('$m_ {jj}$ in TeV')\n",
    "plt.ylabel('(D-B) / $\\sqrt{B}$')\n",
    "\n",
    "# Add a grid\n",
    "plt.grid()\n",
    "\n",
    "# Plot the deficits in red\n",
    "ax.bar(x, minus, width=width, color='salmon', edgecolor=['r']*len(x))\n",
    "\n",
    "# Plot the excesses in green\n",
    "ax.bar(x, plus, width=width, color='lightgreen', edgecolor=['g']*len(x))\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('Approximation.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest deviation can be seen for $m_{jj}\\approx 8 \\,$TeV with almost 3 standard deviations. For all other bins, the deviation is below two standard deviations.\n",
    "\n",
    "Since the Poisson distribution approximates the Gaussian distribution for high-populated bins, this way of plotting is valid for a large background $B$. However, it fails for bins with only a few entries. No reliable statement about the significance can be made for bins with only a few entries. For bins from $m_{jj}>5.874\\,$TeV  the approximation is  not valid anymore. For data sets with only high-populated bins in the interesting  range of the observable, this way is easy and efficient to use. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Statistical Significance\n",
    "\n",
    "In order to plot the differences between the data and the expectations, the statistical significance is considered as a metric to estimate excesses and deficits. Now, the statistical significance is calculated for data with follows the Poisson distribution. A more detailed explanation of the calculation can be read in the written elaboration. \n",
    "\n",
    "Two important parameters are used: Poisson p-value and related z-value.\n",
    "\n",
    "The **p-value** can be defined bin-by-bin as the probability of finding a deviation from the chosen background model that is at least as big as the one observed in data. In this case, the chosen background model is a Poisson distribution with a mean equal to the number of events estimated by the fit.\n",
    "The Poisson p-value is given by\n",
    "\\begin{equation}\\label{eq:poissonpvalue}\n",
    "   \\text{p-value} =\n",
    "   \\begin{cases}\n",
    "     \\sum\\limits_{n=D}^{\\infty} \\frac{B^n}{n!} e^{-B} = 1-\\sum\\limits_{n= 0}^{D-1} \\frac{B^n}{n!} e^{-B}   & \\text{for } D > B \\\\\n",
    "     \\sum\\limits_{n= 0}^{D} \\frac{B^n}{n!} e^{-B}  & \\text{for } D \\leq B\n",
    "   \\end{cases}\n",
    "\\end{equation}\n",
    "where due to the summation from $0$ to $D$ all possible outcomes are considered for deficits $D \\leq B $. The sum for excesses $D > B$ counts until infinity since one bin could have any number of events. \n",
    "\n",
    "In order to calculate the sums, the upper regularized Gamma function $Q(D,B) =  \\Gamma(D,B)/ \\Gamma(D)$  with\n",
    "$$\\Gamma(D,B)  =   \\int_B^\\infty t^{D-1} \\mathrm{e}^{-t}\\,\\mathrm dt$$\n",
    "is used. \n",
    "\n",
    "\n",
    "The **z-value** is the deviation at the right of the mean of a Gaussian distribution in units of standard deviations equivalent to the p-value. It is directly related to the p-value and is calculated with \n",
    "$$ \\text{p-value} = \\int_{\\text{z-value}}^\\infty \\frac{1}{\\sqrt{2\\pi}} \\cdot e^{- \\frac{x^2}{2}}\\, \\text{d}x \\;\\; \\leftrightarrow \\;\\; \\text{z-value} = \\sqrt{2} \\cdot \\operatorname{erf}^{-1}(1-2\\cdot \\text{p-value})$$\n",
    "with the inverse errorfunction $\\operatorname{erf}^{-1}$.\n",
    "\n",
    "The relationship is depicted below.  The significance gets negative for p-value$>0.5$ so that the relation does not work anymore. Therefore, only bins with p-value $ \\leq 0.5$ are considered in the following hypothesis tests. A p-value of $0.5$ is related to z-value $=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "errfunction= plt.figure()\n",
    "\n",
    "p = np.arange(0, 0.5, 0.01)\n",
    "null = p*0\n",
    "l = np.arange(0.5, 1.2, 0.01)\n",
    "plt.grid(True, which=\"both\")\n",
    "z=2**(1./2)*scipy.special.erfinv(1-2.*p)\n",
    "z2=2**(1./2)*scipy.special.erfinv(1-2.*l)\n",
    "plt.plot(p,z, label ='z-value for p-value < 0.5')\n",
    "plt.plot(l,z2,label ='z-value for p-value $\\geq$ 0.5' )\n",
    "plt.axvline(0.5, color='k')\n",
    "plt.hlines(0, 10**(-3), 1.2, color='k')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('p-value')\n",
    "plt.ylabel('z-value')\n",
    "plt.xlim((10**(-2),1))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('errorfunction.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The significance level measures the magnitude of deviations between the results and the model. For an excess, the significance is defined positive and for a deficit negative.  \n",
    "In physics, an evidence is given for a deviation of z-value $\\geq 3$. A new discovery can be proclaimed if the deviation is significant with z-value $ \\geq 5$. Further interpretations are shown in the following table.\n",
    "\n",
    "| z-value | $ \\geq 0$  |$ < 0$| $ 5$  |$1$ | \n",
    "|------|------|------|------|------|------|------|\n",
    "|  p-value   | $\\leq 0.5$|$> 0.5$  | $2.87\\cdot 10^{-7}$| 0.15 |\n",
    "||deviations|no deviations|discovery threshold| 1-sigma statistical fluctuation|\n",
    "\n",
    "The code calculating the p- and z-value has been translated into python from the supporting documentation of [arXiv:1111.2062](https://arxiv.org/abs/1111.2062)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculation the Poisson p-value with the incomplete gamma function\n",
    "def pvalue(D,B):\n",
    "    if D>B :                                  # For an excess\n",
    "        p = scipy.special.gammainc(D, B) \n",
    "    else :                                    # For a deficit          \n",
    "        p= 1-scipy.special.gammainc(D+1, B)   \n",
    "    return p\n",
    "\n",
    "# Function for calculation the significance (z-value) with the inverse errorfunction\n",
    "def zvalue(p):\n",
    "    if y[i]>yb[i]:                                  # For an excess\n",
    "        z=2**(1./2)*scipy.special.erfinv(1-2.*p)\n",
    "    else:                                           # For a deficit, the negative significance is calculated  \n",
    "        z= -2**(1./2)*scipy.special.erfinv(1-2.*p)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig=[]           # List for significance\n",
    "i=0\n",
    "while i < len(y):\n",
    "    sig.append(zvalue(pvalue(y[i], yb[i])))  # Calculate the zvalue in dependence of the pvalue of y and yb and add it to the list\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result: Statistical  Significance\n",
    "\n",
    "Firstly, a figure without excluding bins with ```p-value``` $>0.5$ is produced. Then, in the second figure,  the bins with ```p-value``` $>0.5$ are excluded.\n",
    "\n",
    "The deviations of the data set from the background model are between approximately +1.5 and -2.4 standard deviations. \n",
    "Evidence is accepted for 3 standard deviations and a discovery is proclaimed for a significance higher than 5 standard deviations. Considering the bin-by-bin analysis, no significant excess or deficit is observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all significance values\n",
    "figsig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "# Make the labels\n",
    "plt.xlabel('$m_ {jj}$ in TeV')\n",
    "plt.ylabel('Significance')\n",
    "\n",
    "# Make a grid\n",
    "plt.grid()\n",
    "\n",
    "# Plot the bars in the histogram\n",
    "ax.bar(x, sig, width=width, color='plum', edgecolor=['m']*len(x))\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('Significanceall.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the cut p-value $<0.5$ is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the cut pvalue<0.5\n",
    "\n",
    "# Determine all pvalues<0.5\n",
    "sigp=[]\n",
    "i=0\n",
    "while i < len(y):\n",
    "    if pvalue(y[i], yb[i])<0.5:                    # For pvalues<0.5 ...\n",
    "        sigp.append(zvalue(pvalue(y[i], yb[i])))   # ... add the pvalue\n",
    "        i=i+1\n",
    "    else:                                          # For pvalues>0.5...\n",
    "        sigp.append(0)                             # ... add pvalue=0\n",
    "        i=i+1\n",
    "\n",
    "#Make the plot\n",
    "figsigp = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "# Label the axis\n",
    "plt.xlabel('$m_ {jj}$ in TeV')\n",
    "plt.ylabel('Significance')\n",
    "\n",
    "# Make a grid\n",
    "plt.grid()\n",
    "\n",
    "# Plot the bars in the histogram \n",
    "ax.bar(x, sigp, width=width, color='c', edgecolor=['darkcyan']*len(x))\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('Significance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Comparison of the Bin-by-Bin Analyze\n",
    "The difference between the ways of depicting is recapped in the following part.\n",
    "\n",
    "The absolute and relative difference cannot directly be compared with the approximation and the significance since the unit is not in standard deviations. \n",
    "For large bin populations, the absolute difference is very high and drops down to nearly zero for the low-populated bins. The relative difference shows a reversed behavior. Highly populated bins have a relative difference of almost zero and the relative difference increases rapidly for low populated bins. By comparing those characteristics with the results of the other techniques, it becomes clear that both methods cannot be used for reliably quantifying  excesses or deficits.\n",
    "\n",
    "The differences between the approximation of the significance  $(D-B) / \\sqrt{B}$ and the two ways of the significance are shown below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the different ways in one graph in order to compare the techniques\n",
    "\n",
    "figall = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "# Label the axis\n",
    "plt.xlabel('$m_ {jj}$ in TeV')\n",
    "plt.ylabel('Significance')\n",
    "\n",
    "# Make a grid\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "# Add the Gaussian approximation of the significance\n",
    "ax.bar(x, app, width=width, alpha = 0.3, ls='dotted',color='salmon', edgecolor=['r']*len(x), label='Approximation (D-B) / $\\sqrt{B}$')\n",
    "\n",
    "# Add the significance\n",
    "ax.bar(x, sig, width=width, alpha = 0.3,  color='plum', edgecolor=['m']*len(x), label='Significance')\n",
    "\n",
    "# Add the significance with pvalue < 0.5\n",
    "ax.bar(x, sigp, width=width, alpha = 0.3, ls='-.', color='c', edgecolor=['darkcyan']*len(x), label='Significance with p<0.5')\n",
    "\n",
    "# (Optional) Add the relative difference\n",
    "# ax.bar(x, rel, width=width, alpha = 0.3, ls='dashed', color='yellow', edgecolor=['gold']*len(x), label='Relative Difference (D-B) / B')\n",
    "\n",
    "# (Optional) Add the absolute difference (consider scaling)\n",
    "# ax.bar(x, absolute, width=width, alpha = 0.3, color='springgreen', edgecolor=['g']*len(x), label='Absoulte difference D-B')\n",
    "\n",
    "\n",
    "# Include the legend\n",
    "plt.legend()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('DifferencePlots.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above confirms that the approximation of the significance $\\frac{D-B}{\\sqrt{B}}$ is a quite good approximation for large populated bins, here in the range of $1.1 - 5.874\\,$ TeV. For lower populations in the bins and here subsequently for higher invariant dijet masses $m_{jj}$, the approximation breaks down.  \n",
    "For p-value $>0.5$, the relationship between the p- and the z-value breaks down and would give a z-value in the opposite direction. \n",
    "Thus,  the significance performed with the cut p-value $<0.5$ gives the most accurate result for the determining the deviations in the bin-by-bin analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. The final figure including data, background and histograms###\n",
    "The final figure includes data and expectation in one graph. In subplots the different methods of determining the differences can be presented. The significance and the relative difference are already shown. Other methods like the approximation can easily be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a final plot with all important comparisons\n",
    "\n",
    "# Set up the axes with gridspec\n",
    "result = plt.figure()\n",
    "grid = plt.GridSpec(12, 4, hspace=0.2, wspace=0.2)   # A grid 12x5 for subplots\n",
    "\n",
    "# Main plot\n",
    "main_ax = result.add_subplot(grid[:-4, :], xticklabels=[] )   # Main plot\n",
    "\n",
    "#Properties of the main plot\n",
    "plt.grid()\n",
    "plt.ylabel('Events/Bin')\n",
    "#plt.xlabel('$m_ {jj}$ in TeV')\n",
    "\n",
    "## Histogram plots:\n",
    "\n",
    "#Significance\n",
    "x_hist = result.add_subplot(grid[-4, :], xticklabels=[1,2,3,4,5,6,7,8,9],sharex=main_ax)\n",
    "# Properties of the significance plot\n",
    "plt.grid()\n",
    "plt.ylim((-2.5,2.5))\n",
    "plt.ylabel('Sign.')\n",
    "\n",
    "# Relative Difference\n",
    "x_hist2 = result.add_subplot(grid[-3, :], xticklabels=[1,2,3,4,5,6,7,8,9],sharex=main_ax)\n",
    "\n",
    "# Properties of the relative difference plot\n",
    "x_hist2.yaxis.set_label_position(\"right\")       # Label on the right side\n",
    "x_hist2.yaxis.tick_right()                      # Ticks on the right side\n",
    "plt.grid()                                      # Make a grid\n",
    "plt.ylabel('(D-B)/B')                           # Label the yaxis\n",
    "\n",
    "\n",
    "# (Optional1) Gaussian approximation of the significance\n",
    "#x_hist3 = result.add_subplot(grid[-2, :], xticklabels=[1,2,3,4,5,6,7,8,9],sharex=main_ax)\n",
    "# Properties of the relative difference plot\n",
    "#plt.grid()\n",
    "#plt.ylabel('(D-B)/sqrt(B)')\n",
    "\n",
    "# (Optional2) Absolute difference\n",
    "#x_hist4 = result.add_subplot(grid[-1, :], xticklabels=[1,2,3,4,5,6,7,8,9],sharex=main_ax)\n",
    "# Properties of the relative difference plot\n",
    "#x_hist4.yaxis.set_label_position(\"right\")\n",
    "#x_hist4.yaxis.tick_right()\n",
    "#plt.grid()\n",
    "#plt.ylabel('(D-B)')\n",
    "\n",
    "\n",
    "# x label for the last histogram plot\n",
    "plt.xlabel('$m_ {jj}$ in TeV')\n",
    "\n",
    "### Filling with data\n",
    "\n",
    "# Main plot: Data and background\n",
    "main_ax.errorbar(x,y, yerr=yderr, fmt='ko', label='Data', markersize=2.1, elinewidth=0.8)\n",
    "main_ax.step(x, yb, 'r', linewidth= 0.5, data=None, where='mid', label='Background fit')\n",
    "\n",
    "#ax.text(4,10**5+10**4,'$\\sqrt{s} = 13\\,$TeV, 37.0 fb$^{-1}$')\n",
    "#main_ax.set_xscale(\"log\", nonposx='clip')\n",
    "main_ax.set_yscale(\"log\", nonposy='clip')\n",
    "\n",
    "## Filling the histograms:\n",
    "\n",
    "# Significance\n",
    "x_hist.bar(x, sigp, width=width, alpha = 1, ls='-', color='r', edgecolor=['k']*len(x), label='Significance with p<0.5')\n",
    "\n",
    "# Relative difference\n",
    "x_hist2.bar(x, rel, width=width, alpha = 1, ls='-', color='yellow', edgecolor=['gold']*len(x), label='Relative Difference')\n",
    "\n",
    "# (Optional1) Gaussian approximation of the significance\n",
    "#x_hist3.bar(x, app, width=width, alpha = 1, ls='-', color='plum', edgecolor=['m']*len(x), label='Approximation')\n",
    "\n",
    "# (Optional2) Absolute difference\n",
    "#x_hist4.bar(x, absolute, width=width, alpha = 1, ls='-', color='lightgreen', edgecolor=['green']*len(x), label='Absolute difference')\n",
    "\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('Results.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The invariant mass $m_{jj}$ of dijet events of a data set recorded in 2015 and 2016 with the ATLAS detector in proton-proton collisions at $\\sqrt{s} = 13 \\,$TeV corresponding to an integrated luminosity of $37\\,\\text{fb}^{-1}$ were investigated. The Jupyter notebook comparing the data and the background prediction was validated by comparing the results with the results of the ATLAS collaboration.\n",
    "\n",
    "Consequently, no significant local excess between the measured data and the predicted background is observed. For dijet events, the deviations of all the single bins lay below 2.4 standard deviations. Had it been three standard deviations, this would have been regarded as evidence.\n",
    "\n",
    "The focus of this notebook is set on the statistical analysis of comparing a data set with a background prediction. Besides bin-by-bin analysis techniques, hypothesis hyper tests are introduced to get a global analysis of the spectrum. \n",
    "\n",
    "In the first part of the analysis, different ways of bin-by-bin comparisons are applied and compared. It was shown that methods as depicting the absolute difference or the relative difference between data and background are not comparable with the significance. Additionally, it was shown that the Gaussian approximation of the significance for a Poisson distributed data set is only suitable for high-populated bins with at least 10 entries. As a consequence, the most informative way is to illustrate differences by the calculation of  the significance via the p-value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits\n",
    "\n",
    "The notebook was written by Leonie Hermann as a part of a Bachelor's project during an Erasmus exchange at Lund University, under the supervision of Caterina Doglioni (Lund) and Markus Schumacher (Freiburg). Support and troubleshooting was provided by Florido Paganelli (Lund), Matteo Bauce, the Anaconda community and the ROOT notebook community."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
